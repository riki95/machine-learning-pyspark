{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "### Import libraries\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.session import SparkSession\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nfrom pyspark.context import SparkContext, SparkConf\n\n\n# ### Configure Spark\napp_name = 'HPC Project'\n\n### PySpark session initialization\nconf = SparkConf().setAppName(app_name)\n\nsc = SparkContext.getOrCreate(conf)\nspark = SparkSession(sc)\n# print(sc._conf.getAll())  # Get all the configuration parameters info\n\n\n### Load the source data\ncsv = spark.read.csv('bank_1g.csv', inferSchema=True, header=True, sep=',')", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1548681355268_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-hpcbas.fajxbalhwinevprl1eqakbtxif.ax.internal.cloudapp.net:8088/proxy/application_1548681355268_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn4-hpcbas.fajxbalhwinevprl1eqakbtxif.ax.internal.cloudapp.net:30060/node/containerlogs/container_1548681355268_0008_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "### Select features and label\ndata = csv.select(*(csv.columns[:-1]+ [((col(\"y\")).cast(\"Int\").alias(\"label\"))]))\n# print(data)\n\n\n### Split the data and rename Y column\nsplits = data.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n\n\n### Define the pipeline\nassembler = VectorAssembler(inputCols = data.columns[:-1], outputCol=\"features\")\nprint(\"Input Columns: \", assembler.getInputCols())\nprint(\"Output Column: \", assembler.getOutputCol())\n\nalgorithm = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\npipeline = Pipeline(stages=[assembler, algorithm])", "outputs": [{"output_type": "stream", "name": "stdout", "text": "('Input Columns: ', ['age', 'default', 'balance', 'housing', 'loan', 'duration', 'campaign', 'pdays', 'previous', 'job_admin', 'job_blue-collar', 'job_entrepreneur', 'job_housemaid', 'job_management', 'job_retired', 'job_self-employed', 'job_services', 'job_student', 'job_technician', 'job_unemployed', 'job_unknown', 'marital_divorced', 'marital_married', 'marital_single', 'education_primary', 'education_secondary', 'education_tertiary', 'education_unknown', 'contact_cellular', 'contact_telephone', 'contact_unknown', 'day_1', 'day_2', 'day_3', 'day_4', 'day_5', 'day_6', 'day_7', 'day_8', 'day_9', 'day_10', 'day_11', 'day_12', 'day_13', 'day_14', 'day_15', 'day_16', 'day_17', 'day_18', 'day_19', 'day_20', 'day_21', 'day_22', 'day_23', 'day_24', 'day_25', 'day_26', 'day_27', 'day_28', 'day_29', 'day_30', 'day_31', 'month_apr', 'month_aug', 'month_dec', 'month_feb', 'month_jan', 'month_jul', 'month_jun', 'month_mar', 'month_may', 'month_nov', 'month_oct', 'month_sep', 'poutcome_failure', 'poutcome_other', 'poutcome_success', 'poutcome_unknown'])\n('Output Column: ', 'features')"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "### Tune Parameters\nlr_reg_params = [0.01, 0.5, 2.0]\nlr_elasticnet_param = [0.0, 0.5, 1.0]\nlr_max_iter = [1,5,10]\n\n\n### CrossValidation\nfolds = 2\nparallelism = 3\n\nevaluator=BinaryClassificationEvaluator()\nparamGrid = ParamGridBuilder().addGrid(algorithm.regParam, lr_reg_params).addGrid(algorithm.maxIter, lr_max_iter).addGrid(algorithm.elasticNetParam, lr_elasticnet_param).build()\n\ncv = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=paramGrid, numFolds=folds).setParallelism(parallelism)\n\n#cv = DagCrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, parallelism=parallelism)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "#### Training\nimport time\ntic = time.time()\n\nmodel = cv.fit(train)\n\ntoc = time.time()\nprint(\"Elapsed time \", toc-tic)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "### Test the Model\nprediction = model.transform(test)\npredicted = prediction.select(\"features\", \"prediction\", \"probability\", \"trueLabel\")\n# print(*predicted.select('prediction', 'trueLabel').collect(), sep='\\n')\nprint('true positives:', predicted.filter('trueLabel == 1').count())\nprint('true negatives:', predicted.filter('trueLabel == 0').count())", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "### Compute Confusion Matrix Metrics\ntp = float(predicted.filter(\"prediction == 1.0 AND trueLabel == 1\").count())\nfp = float(predicted.filter(\"prediction == 1.0 AND trueLabel == 0\").count())\ntn = float(predicted.filter(\"prediction == 0.0 AND trueLabel == 0\").count())\nfn = float(predicted.filter(\"prediction == 0.0 AND trueLabel == 1\").count())\nprecision = tp / (tp + fp)\nrecall = tp / (tp + fn)\ncorrectly_classified = (tp+tn) / predicted.count()\nF1 = 2 * (precision * recall) / (precision + recall)\n\nmetrics = spark.createDataFrame([\n\t\t\t\t\t(\"TP\", tp),\n\t\t\t\t\t(\"FP\", fp),\n\t\t\t\t\t(\"TN\", tn),\n\t\t\t\t\t(\"FN\", fn),\n\t\t\t\t\t(\"Precision\", precision),\n\t\t\t\t\t(\"Recall\", recall),\n\t\t\t\t\t(\"Correctly Classified\", correctly_classified),\n                    (\"F1\", F1)\n\t\t\t\t],\n\t\t\t\t[\"metric\", \"value\"])\n\n### Print Results\nmetrics.show()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}